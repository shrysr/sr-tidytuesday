* Introduction
This repo will contain my code explorations with datasets available via the [[https://github.com/rfordatascience/tidytuesday][#TidyTuesday]] project. The idea is to evolve towards create a document based on literate programming principles, with the code side by side with the exploration / analysis.

The latest project is on top.

** Tools
I use ESS (and Emacs) for all my workflows. This is a single Org document containing each exploration under a heading, and code blocks enclosed within Org-babel headers. Scripts are tangled into the [[/00_scripts/][00_scripts]] folder.

** TODO Export each project to separate Juypyter / Rmd notebooks for easy access.

* Project 2: TV's Golden Age is real [[https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-08][link]]
** Questions
*** How many unique titles per genre?
*** how many shows released every year, for each genre.
*** Find the average rating of each title across seasons
*** Average rating variation across genres. Which genres seem more popular?
*** Average number of seasons and the connection with the average rating
*** Top 10, based on number of seasons and average rating, and bringing out the connection with genre.
** Notes
** Code
#+BEGIN_SRC R :session R
                                        # Loading libraries
library("easypackages")
libraries("tidyverse")
                                        # Reading in the data directly from github
tv_ratings_raw  <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-08/IMDb_Economist_tv_ratings.csv")

tv_ratings_raw %>% skimr::skim()
tv_ratings_raw %>% glimpse()

                                        # Spreading out the genre separated by a comma. Initially started with 6 splits, but the maximum is 3 and there are several with less.
test <- tv_ratings_raw %>%
  separate(genres,
           into = c("genre1_", "genre2_", "genre3_"),
           sep = ',' ,
           remove = FALSE
           )  %>%
  select(ends_with("_")) %>%
  map(unique)


   ##mutate_at(vars(ends_with("_")), funs(as.factor))


all_genres <-  test %>%
  select(ends_with("_")) %>% gather(
                               key = genres_col,
                               value = genre_all,
                               genre1_,
                               genre2_,
                               genre3_,
                               na.rm = TRUE
                             ) %>%
  select(genre_all) %>%
  unique()

all_genres %>%  unique

all_genres %>% spread(
                 key = genre_all
                 )

test2 <- test %>% select_if(vars(ends_with("_"))) %>% map(unique)
test %>% select_if(is.factor)
#+END_SRC
****
* Project 1: Federal R&D spending by agency [[https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-12][link]]
:PROPERTIES:
:CREATED:  <2019-02-25 Mon 14:08>
:END:
** Viz [[https://twitter.com/ShreyasRagavan/status/1100765886892265472][posted on Twitter]]
** Code
#+BEGIN_SRC R :session tt :tangle ./00_scripts/p1_climate_spending.R
                                        # Loading libraries
library("easypackages")
libraries("tidyverse", "tidyquant", "gganimate")

                                        # Reading in data directly from github
climate_spend_raw  <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-12/climate_spending.csv", col_types = "cin")


                                        # This initial conditioning need not have involved the date manipulation, as the year extracted from a date object is still a double.
climate_spend_conditioned <- climate_spend_raw %>%
  mutate(year_dt = str_glue("{year}-01-01")) %>%
  mutate(year_dt = as.Date(year_dt)) %>%
  mutate(gcc_spending_txt = scales::dollar(gcc_spending,
                                           scale = 1e-09,
                                           suffix = "B"
                                           )
         )

climate_spend_dept_y <- climate_spend_conditioned %>%
  group_by(department, year_dt = year(year_dt)) %>%
  summarise(
    tot_spend_dept_y = sum(gcc_spending)) %>%
  mutate(tot_spend_dept_y_txt = tot_spend_dept_y %>%
           scales::dollar(scale = 1e-09,
                          suffix = "B")
         ) %>%
  ungroup()

glimpse(climate_spend_dept_y)

climate_spend_plt_fn <- function(
                               data,
                               y_range_low = 2000,
                               y_range_hi  = 2010,
                               ncol = 3,
                               caption = ""
                               )
{
  data %>%
    filter(year_dt >= y_range_low & year_dt <= y_range_hi) %>%
    ggplot(aes(y = tot_spend_dept_y_txt, x = department, fill = department ))+
    geom_col() +
    facet_wrap(~ year_dt,
               ncol = 3,
               scales = "free_y") +
    theme_tq() +
    scale_fill_tq(theme = "dark") +
    theme(
      axis.text.x = element_text(angle = 45,
                                 hjust = 1.2),
      legend.position = "none",
      plot.background=element_rect(fill="#f7f7f7"),
    )+
    labs(
      title = str_glue("Federal R&D budget towards Climate Change: {y_range_low}-{y_range_hi}"),
                       x = "Department",
                       y = "Total Budget $ Billion",
                       subtitle = "NASA literally dwarfs all the other departments, getting to spend upwards of 1.1 Billion dollars every year since 2000.",
                       caption = caption
    )

}

climate_spend_plt_fn(climate_spend_dept_y,
                     y_range_low = 2000,
                     y_range_hi = 2017,
                     caption = "#TidyTuesday:\nDataset 2019-02-12\nShreyas Ragavan"
                       )

## The remaining code is partially complete and is in place for further exploration planned in the future.

## Code to download all the data.
## fed_rd <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-12/fed_r_d_spending.csv")
## energy_spend <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-12/energy_spending.csv")
## climate_spend <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-12/climate_spending.csv")

## climate_spend_pct_all <- climate_spend_conditioned %>%
##   group_by(year_dt = year(year_dt)) %>%
##   summarise(
##     tot_spend_all_y = sum(gcc_spending)
##   ) %>%
##   mutate(tot_spend_all_y_txt = tot_spend_all_y %>%
##            scales::dollar(scale = 1e-09,
##                           suffix = "B"
##                           )
##          )%>%
##   ungroup() %>%
##   mutate(tot_spend_all_lag = lag(tot_spend_all_y, 1)) %>%
##   tidyr::fill(tot_spend_all_lag ,.direction = "up") %>%
##   mutate(tot_spend_all_pct = (tot_spend_all_y - tot_spend_all_lag)/ tot_spend_all_y,
##          tot_spend_all_pct_txt = scales::percent(tot_spend_all_pct, accuracy = 1e-02)
##          )

#+END_SRC

#+RESULTS:
